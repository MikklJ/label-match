# Label & Match

## Introduction

The repo for matching annotations from distinct video frames. This research has been done by Michael Ji at Blink AI.

This project contains a potential pipeline for annotating images from video frames, using interpolation and bounding-box matching
The interpolation program takes bounding box labels from two frames of a video, and creates autogenerated interpolated bounding boxes for
frames in between those two frames.

The interpolation program matches bounding boxes of the same object and runs linear interpolation for each pair of boxes.

* Much of the siamese neural network portion of this repo is sourced from this Adam Bielski repo: https://github.com/adambielski/siamese-triplet
* The siamese neural network structure/training procedure comes from this paper: https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf
* Note that this project is unfinished. The siamese neural network originally intended to match bounding box labels does not work (see Project Reflection)
    * A conventional, non ML image similairty metric (SSIM) is used as a substitute 

## Project Structure

- config 
    - label-match.yaml: contains settings for loading training data and training the neural network for label matching
- datasets
    - michael_processed: training dataset - contains images (cars, trucks, traffic lights) sourced from images cropped by bounding boxes
    - 2871_labels.json: contains absolute filepaths and labels for a 2871-element labeled subset of michael_processed
- **interpolator
    - interpolator.py: the main interpolation program - takes 2 additional args (paths to label information)
    - bounding_box.py: class definition for bounding boxes (auxiliary to interpolator.py)
    - 1aa36a55-127141c0: a test example of a set of video frames that can be labeled using this project's methodology
- **loader
    - cropped_dataset.py: constructs the training dataset, helps load training image pairs
    - bdd_utils.py: label map for the training dataset, along with other legacy label maps
    - selector.py: unused dataset selector system from https://github.com/adambielski/siamese-triplet
- **models
    - siamese.py: contains the siamese NN layer definition for the siamese network used to match image labels (with triplet network defs)
    - losses.py: contains cost/loss functions for training the siamese neural network
    - utils.py, resnet.py: unused definitions for CNN modules, and a ResNet - https://github.com/adambielski/siamese-triplet
- outputs
    - train: contains siamese NN state_dicts from the training procedure
- runs 
    - contains logs for some training sessions
- utils
    - contains various utility scripts and uncorrected labels, used in the process of creating the cropped training dataset
- nohup.out
    - log of one complete training session (60 epochs, using BCE loss and an alternate version of the siamese NN)
- **train.py
    - trains the siamese neural network output the chance that two images (from cropped_dataset) are from the same class (map in bdd_utils)

## Progress

- [X] Initialize repository with design goals (10 minutes)
- [X] Learn using gcloud resources and transition code to google cloud (1/2 day)
- [X] Learn using git(add&commit&pull&push pipeline) and transition code to bitbucket so two people can work on the project at the same time. (1/2 day)
- [X] **Prepare Dataloader for getting cropped images by using the bboxes of bdd-dataset during training (let's call this dataset "Cropped")** (1 day)
- [X] **Implement siamese or triplet networks** (3 days)
- [X] **Prepare siamese loss (or triplet loss)** (1-2 days)
- [ ] **Training of siamese networks can be done online, but for best results train the network with considerable amount of cropped images** (3 days)
- [ ] **Formalize report of accuracies and validate against ground truth labels, you can use your previous labels for this** (1 day)
- [X] **Integrate Siamese network into the annotation workflow** (1 day)
- [ ] **Demo Results** (1 day)
- [ ] (optional) visualize shallow model on the Cropped Database (1/2 day)


## Project Reflection
* Issues
    * the training dataset was imbalanced (some classes like black cars appear hundreds of times, while others appear less than a dozen times)
    * different labeled images were of very different image dimensions, and were not very high quality
    * the neural network's structure was likely suboptimal
* Potential Improvements
    * Build a GUI that allows the user to check whether the label matches are correct, and change matches if necessary
    * Use an existing, high quality dataset like ImageNet to train the SNN, and generate better results


## Other thoughts and notes 
* we will make a new directory automatically each time we run ```train``` which contains for that run
    * config
    * tensorboard log
    * saved models (the correct way https://pytorch.org/docs/master/notes/serialization.html?highlight=save)
* we will use the same data augmentation and image resize for all models. Learning rate can be model dependent.
* we will use git to keep each other up-to-date on implementation successes/failures

## Tools
* Virtualenv (Python virtual environments with all dependencies installed)
* Python 3, PyTorch 1.1.0
* PyTorch Imagenet pretrained backbones - https://github.com/Cadene/pretrained-models.pytorch
* PIL SSIM image similarity package - https://pypi.org/project/SSIM-PIL/
* Google Cloud for training - https://cloud.google.com
* TensorboardX for viz (browser is ideal) - https://github.com/lanpa/tensorboardX
* yaml for config - (example usage: https://github.com/NVlabs/MUNIT/blob/master/configs/demo_edges2handbags_folder.yaml)
